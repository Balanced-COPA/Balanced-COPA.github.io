---
layout: single
toc: true
---
<h1>Summary</h1>
<p>
  <blockquote>Pretrained language models, namely BERT, have shown large improvements in the
    commonsense
    reasoning
    benchmark COPA.
    However, recent work found that many improvements in benchmarks of natural language
    understanding
    are not due to models
    learning the task, but due to their increasing ability to exploit superficial cues, such as
    tokens
    that occur more often
    in the correct answer than the wrong one. Is BERT's good performance on COPA also caused by
    this?
    We find superficial cues in COPA, as well as evidence that BERT exploits these cues.
    To remedy this problem, we introduce Balanced COPA, an extension of COPA that does not suffer
    from
    easy-to-exploit
    single token cues.
    We analyze BERT's performance both on original and Balanced COPA, finding that BERT relies on
    superficial cues when they
    are present, but still achieves comparable performance once they are made ineffective,
    suggesting
    that BERT learns the
    task to a certain degree when forced to.
  </blockquote>
</p>

<h1>Choice of Plausible Alternatives (COPA)</h1>
<p>Given a premise, such as <em>The man broke his toe</em>,
  <a href="http://people.ict.usc.edu/~gordon/copa.html" rel="nofollow" target="_blank">COPA</a>
  requires choosing the more plausible, causally related alternative, in this case
  either: because
  <em>He got a hole in his sock</em> (wrong) or because <em>He dropped a hammer
    on his foot</em> (correct).
  To test whether COPA contains superficial cues, we conduct a dataset ablation in which we provide only partial input
  to
  the model.
  Specifically, we provide only the two alternatives, but not the premise, which makes solving the task impossible and
  hence should reduce the model to random performance.
  However, we observe that a model trained only on alternatives performs considerably better than random chance and
  trace
  this result to an unbalanced distribution of tokens between correct and wrong alternatives.
  Further analysis reveals that finetuned BERT and RoBERTa perform very well (80.5 percent
  accuracy) on <em>easy</em> instances containing superficial cues, but worse (65.8 percent) on <em>hard</em> instances
  without such simple cues.

  To prevent models from exploiting superficial cues in COPA, we introduce
  <strong>Balanced COPA</strong></p>

<h1>Balanced COPA</h1>
<p>Balanced COPA contains one additional,
  <em>mirrored</em> instance for each original training instance.
  This mirrored instance uses the same alternatives as the corresponding original
  instance, but introduces a new premise which matches the <em>wrong</em> alternative
  of the original instance, e.g. <em>The man hid his feet</em>, for which the correct
  alternative is now because <em>He got a hole in his sock</em>.
  Since each alternative occurs exactly once as correct answer and exactly once as
  wrong answer in Balanced COPA, the lexical distribution between correct and wrong
  answers is perfectly balanced, i.e., superficial cues in the original alternatives
  have become uninformative.

  Balanced COPA allows us to study the impact of the presence or absence of
  superficial cues on model performance.</p>
<!--
<h1>Examples</h1>
<p>Coming soon</p>
<h1>Results</h1>
<p>Coming soon</p>
<h1>Attention MAPs</h1>
<p>Coming soon</p>
-->